{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668588d8-5baa-447b-afab-dfaed7febd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional, Callable, List\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7960a2-cee9-4673-86c2-5b1789f74d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../dataset')\n",
    "sys.path.append('../graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342881c0-01d1-4643-9853-aafc093adb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_image_graph_dataset import TextImageGraphDataset\n",
    "from torchvision.transforms import Compose, Normalize, Resize, InterpolationMode\n",
    "from image_graph_dataset import ImageGraphDataset, rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250e343a-c4e9-4af1-8a89-5ce2fd0e7e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/deep/u/danieljm/miniconda3/envs/img-graph/lib/python3.8/site-packages/torch_geometric/data/dataset.py:159: UserWarning: The `pre_filter` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-fitering technique, make sure to delete '{self.processed_dir}' first\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextImageGraphDataset(220736)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_root = '/deep/group/img-graph/radgraph_tokens'\n",
    "image_root = '/deep/group/mimic-cxr-small/original'\n",
    "graph_root = '/deep/group/img-graph'\n",
    "\n",
    "input_resolution = 320\n",
    "transform = Compose([\n",
    "    Normalize((101.48761, 101.48761, 101.48761), (83.43944, 83.43944, 83.43944)),\n",
    "])\n",
    "    \n",
    "dset = TextImageGraphDataset(\n",
    "        name='train',\n",
    "        text_root=text_root,\n",
    "        image_root=image_root,\n",
    "        graph_root=graph_root,\n",
    "        image_pre_transform=rescale,\n",
    "        image_transform=transform,\n",
    "    )\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012ad244-0c48-471d-a760-44d139903ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type clip_text_model. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at openai/clip-vit-base-patch32 were not used when initializing CLIPTextModel: ['vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4369422c539429696d0f28a2b0e96ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220736 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: len(tokens) = 32 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 42 != 41 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 25 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 38 != 39 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 24 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 21 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 28 != 27 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 57 != 54 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 57 != 54 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 53 != 51 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 28 != 27 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 45 != 44 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 19 != 18 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 35 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 71 != 69 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 57 != 55 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 28 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 29 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 13 != 12 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 31 != 32 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 66 != 68 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 49 != 50 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 42 != 41 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 28 != 27 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 13 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 46 != 45 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 55 != 57 = graph.x.shape[0]\n",
      "ERROR: image directory /deep/group/mimic-cxr-small/original/p14/p14036332/s59670274 does not exist! Skipping...\n",
      "ERROR: len(tokens) = 38 != 37 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 14 != 13 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 35 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 25 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 46 != 48 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 13 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 23 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 57 != 54 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 18 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 38 != 37 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 18 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 29 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 21 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 17 != 16 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 35 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 19 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 34 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 29 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 21 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 28 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 40 != 41 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 41 != 43 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 41 != 43 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 55 != 54 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 44 != 45 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 37 != 36 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 32 != 33 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 39 != 44 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 40 != 38 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 47 != 49 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 54 != 55 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 33 != 40 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 47 != 41 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 12 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 8 != 14 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 52 != 51 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 53 != 52 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 35 != 34 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 29 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 18 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 25 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 34 != 33 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 23 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 42 != 44 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 29 != 27 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 40 != 56 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 25 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 16 != 15 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 23 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 17 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 61 != 64 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 8 != 7 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 66 != 67 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 96 != 95 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 29 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 33 != 34 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 21 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 25 != 24 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 35 != 39 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 60 != 61 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 16 != 17 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 34 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 43 != 44 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 23 != 22 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 53 != 55 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 52 != 51 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 24 != 22 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 24 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 14 != 15 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 23 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 32 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 18 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 19 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 44 != 43 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 30 != 29 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 25 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 10 != 9 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 15 != 14 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 33 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 17 != 16 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 23 != 22 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 24 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 37 != 36 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 41 != 43 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 18 != 20 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 39 != 40 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 32 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 36 != 35 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 60 != 55 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 26 != 28 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 88 != 90 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 37 != 36 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 40 != 39 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 32 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 35 != 36 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 22 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 61 != 60 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 14 != 13 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 29 != 31 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 34 != 33 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 74 != 70 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 32 != 30 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 25 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 27 != 26 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 25 != 24 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 33 != 32 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 78 != 80 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 20 != 21 = graph.x.shape[0]\n",
      "ERROR: len(tokens) = 46 != 44 = graph.x.shape[0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'openai/clip-vit-base-patch32'\n",
    "encoder = CLIPTextModel.from_pretrained(model_name).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "for data in tqdm(dset):\n",
    "    new_dir = '/'.join(data.pid.split('/')[:2])\n",
    "    new_dir = os.path.join('/deep/group/img-graph/radgraph_mean_BERT_embeddings', new_dir)\n",
    "    Path(new_dir).mkdir(parents=True, exist_ok=True)\n",
    "    new_file = data.pid.split('/')[2][:-4] + '.pt'\n",
    "    new_file = os.path.join(new_dir, new_file)\n",
    "    \n",
    "    if os.path.isfile(new_file):  # file already exists so continue\n",
    "        continue\n",
    "    \n",
    "    inputs = tokenizer(data.tokens, padding=True, return_tensors=\"pt\").to(device)\n",
    "    # print('INPUTS DEVICE', inputs.device)\n",
    "    outputs = encoder(**inputs)\n",
    "    pooled_output = outputs.pooler_output\n",
    "    avg_output = torch.mean(pooled_output, dim=0)  # average across tokens within a graph\n",
    "    # print(data.pid, avg_output.shape)\n",
    "    torch.save(avg_output, new_file)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80013886-a3bd-488f-9bff-3de1ffbcb176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24fa36-6e5f-4216-9838-ef14d3a41550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34896d7-2366-4a27-a0cb-56ddd746754a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
